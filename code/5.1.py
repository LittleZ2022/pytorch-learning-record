# 神经元：输入的线性变换（例如，输入乘以一个数（weight）），再加上一个常数（bias），再经过一个固定的非线性函数（激活函数）
# o=f(wx + b)，其中 x 为输入，w为权重或缩放因子，b为偏置或偏移。f是激活函数，在此处设置为双曲正切（tanh）函数。
# 通常，x以及o可以是简单的标量，也可以是向量（包含许多标量值）。
# 类似地，w可以是单个标量或矩阵，而 b 是标量或向量（输入和权重的维度必须匹配）。
# 之前的线性模型与你将要使用的深度学习模型之间的重要区别是误差函数的形状。线性模型和误差平方损失函数具有凸的具有明确定义的最小值的误差曲线。
# 如果你要使用其他方法（即非梯度下降的方法），则可以自动地求出这个明确的最小值。而（使用梯度下降）参数更新则试图尽可能地估计出这个最小值。
# 即使使用相同的误差平方损失函数，神经网络也不具有凸误差曲面这个属性。你尝试优化的每个参数都没有一个明确正确的答案。
# 相反，你尝试优化所有协同工作的参数以产生有用的输出。由于有用的输出只会逼近真实值，因此会有一定程度的不完美。
# 神经网络具有非凸误差曲面主要是因为激活函数。组合神经元来逼近各种复杂函数的能力取决于每个神经元固有的线性和非线性行为的组合。

# 激活函数：将先前线性运算的输出聚集到给定范围内
import math
print(math.tanh(-2.2))
print(math.tanh(0.1))
print(math.tanh(2.5))

'''-0.9757431300314515
0.09966799462495582
0.9866142981514303'''
